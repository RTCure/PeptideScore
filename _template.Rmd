---
title: "PeptideScore Template"
author: "Kathryn Steel, Yann Abraham"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    code_folding: show
    toc: yes
    toc_float: yes
    toc_depth: 4
version: 0.1.0
---

```{r setup}
library(targets)
tar_unscript()

library(flowWorkspace)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(forcats)
library(ggplot2)

library(tarchetypes)

knitr::opts_chunk$set(warning=FALSE,
                      echo=TRUE,
                      fig.width=8,
                      fig.height=6,
                      fig.retina=1,
                      fig.keep='high',
                      fig.align='center',
                      knitr.table.format = "html")

theme_set(theme_light(base_size = 16))
```

```{targets pipeline, tar_globals = TRUE}
options(tidyverse.quiet = TRUE)
tar_option_set(packages = c("flowCore","flowWorkspace",
                            "dplyr","tidyr","stringr","forcats","tibble",
                            "ggplot2","Rtsne","Radviz",
                            "igraph","rmarkdown"))
```

# Goal

this is a first go at a reproducible and shareable pipeline for PeptideScoe - not everything is settled yet!

the pipeline is broken down into 4 stages

# Stage 1 - Loading the data

## load the `GatingSet` from FlowJo

Specify the path to the FlowJo workspace that will be used :

```{targets ws.file}
tar_target(ws.file,
             "/path/to/flowjo.wsp",
             format = "file")
```

Specify the path where the `GatingSet` object will be stored :

```{targets output.dir, tar_simple=TRUE}
"/path/to/output/folder"
```

Specify the name of the `GatingSet` :

```{targets gs.id, tar_simple=TRUE}
"test_gs"
```

We can now load and parse the FlowJo workspace and save it to the file system:

```{targets gs.file, tar_interactive=FALSE}
tar_target(gs.file,{
  unlink(file.path(output.dir,gs.id),
         recursive = TRUE)
  ws <- openWorkspace(ws.file)
  gs <- parseWorkspace(ws,name = "gating")
  save_gs(gs,
          file.path(output.dir,gs.id))
  file.path(output.dir,gs.id)
})
```

## Preparing annotations

```{r load_gs_interactive}
tar_noninteractive(tar_load(gs.file))
gset <- load_gs(gs.file)
```

We start with sample annotation:

```{r prepare_samples}
pData(gset) %>%
  select(`FCS Filename` = name) %>%
  rownames_to_column("name") %>%
  mutate(Total = str_extract(name,"_[0-9]{4,}"),
         Total = str_replace(Total,"_",""),
         Total = as.numeric(Total)) %>% 
  as_tibble()
```

Once sample annotation works replace it in the following code chunk:

```{targets annots, tar_simple=TRUE}
gset <- load_gs(gs.file)
.annots <- pData(gset) %>%
      select(`FCS Filename` = name) %>%
      rownames_to_column("name") %>%
      mutate(Total = str_extract(name,"_[0-9]{4,}"),
             Total = str_replace(Total,"_",""),
             Total = as.numeric(Total))
rownames(.annots) <- .annots$name
.annots
```

Next we prepare for marker annotation; the following table shows a list of markers present in the dataset:

```{r}
as_tibble(pData(parameters(flowData(gset)[[1]])),
          rownames = "id")
```

In the next block we list the `desc` of markers that should be ignored:

```{targets params_ref_markers, tar_simple=TRUE}
"(LD|CD8|CFSE|GPR56|CD39)"
```

The next block builds marker annotation:

```{targets markers}
tar_target(markers,{
  gset <- load_gs(gs.file)
  as_tibble(pData(parameters(flowData(gset)[[1]])),
            rownames = "id") %>% 
    mutate(id = paste0(id,"S"),
           type = case_when(
             str_detect(desc,
                        params_ref_markers) ~ "reference",
             is.na(desc) ~ "Flow",
             TRUE ~ "functional"
           ),
           desc_orig = desc,
           desc = ifelse(is.na(desc),name,desc),
           isTransformed  = name %in% names(getTransformations(gset[[1]])))
})
```

The next block lists available populations:

```{targets pops}
tar_target(pops,{
  gset <- load_gs(gs.file)
  .pops <- lapply(getNodes(gset),function(i) {
    pop <- lapply(sampleNames(gset), function(j) {
      getIndiceMat(gset[[j]],i)
    })
    pop <- do.call(rbind,pop)
    return(pop)
  })
  .pops <- do.call(cbind,.pops)
})
```

Available populations are 

```{r available_pops, results="asis"} 
tar_noninteractive(tar_load(pops))
ksink <- lapply(colnames(pops),function(x) cat("\t*",x,"\n"))
```

In the next block you can specify which population corresponds to the data you want to analyze:

```{targets cur.pop, tar_simple=TRUE}
"/path/to/population"
```

## Extract intensity matrix

Based on all information we can extract the intensity matrix and turn it into an annotated `data.frame`:

```{targets div.df}
tar_target(div.df,{
  gset <- load_gs(gs.file)
  .files <- with(annots,rep(name,Total))
  
  cur.pop.index <- lapply(gset,getIndices,cur.pop)
  cur.pop.index <- unlist(cur.pop.index)
  
  .div.df <- getData(gset,cur.pop)
  as_tibble(fsApply(.div.df,exprs)) %>% 
    select(with(markers,name[type=="functional"])) %>% 
    rename_with(~ with(markers,desc[type=="functional"])) %>% 
    mutate(name = .files[cur.pop.index]) %>% 
    left_join(annots,
              by = "name") %>% 
    group_by(name) %>% 
    mutate(id = paste0(name,seq(length(name))))
})
```

```{r show_df}
tar_noninteractive(tar_load(div.df))
div.df
```

# Stage 2 - Computing distance

Using all functional markers in the dataset we compute the cosine similarity beteween all cells:

```{targets div.dist, tar_interactive=FALSE}
tar_target(div.dist,{
  .mat <- as.matrix(div.df %>% 
                      ungroup() %>% 
                      select(with(markers,desc[type=="functional"])))
  rownames(.mat) <- div.df$id
  .mat <- t(.mat)
  crossprod(.mat)/(sqrt(tcrossprod(colSums(.mat^2))))
})
```

Next we generate a rank matrix based on the distance matrix:

```{targets div.rank, tar_interactive=FALSE}
tar_target(div.rank,{
    .div.rank <- div.dist
    
    diag(.div.rank) <- 0
    
    .div.rank <- apply(.div.rank,1,rank,ties.method = "first")
    .div.rank <- nrow(div.dist)+1-.div.rank
    t(.div.rank)
  })
```

# Stage 3 - Preparing the graph

## Find optimal *k*

To identify the optimal *k* we suggest starting from the square root of the number of cells, then exploring a number of critical value around that one.

We start by specifying a range of *k* values to test:

```{targets ks, tar_simple=TRUE}
seq(100,200,by = 25)
```

Next we compute the corresponding graphs, and extract the communities using the Louvain algorithm

```{targets comms, tar_interactive=FALSE}
tar_target(comms,{
  .adj <- div.dist
  .adj[div.rank>ks] <- 0
  .graph <- graph_from_adjacency_matrix(.adj,
                                        mode = "undirected",
                                        weighted = TRUE)
  cluster_louvain(.graph)
},
pattern = map(ks)
)
```

Finally we extract the following summary statistics:

  * Number of communities
  * Modularity
  * Gini coefficient of community size

```{targets mods, tar_interactive=FALSE}
tar_target(mods,{
  ## compute a gini coefficient for community size
  .sizes <- sizes(comms)
  .n <- length(.sizes)
  
  tibble(k = ks,
         modularity = modularity(comms),
         Gini = sum(outer(.sizes,
                          .sizes,
                          FUN=function(x,y){
                            abs(x-y)})) / (2 * .n * sum(.sizes)),
         N = length(comms))
},
pattern = map(ks,comms))
```

We visualize the different parameters in the following plot:

```{r show_modularity}
tar_noninteractive(tar_load(mods))
mods %>% 
  mutate(k = factor(as.numeric(k))) %>% 
  ggplot(aes(x = N,
             y = modularity))+
  geom_path(size = 1.5,
            col = "grey80")+
  geom_text(aes(label = k,
                size = Gini))
```

## Compute graph

We define the optimal k as a parameter in the following block:

```{targets params_k, tar_simple=TRUE}
150
```

Then build the graph accordingly:

```{targets div.graph, tar_interactive=FALSE}
tar_target(div.graph,{
  ## build the adjacency matrix
  .adj <- div.dist
  .adj[div.rank>params_k] <- 0
  
  ## build the graph
  graph.adjacency(.adj,
                  mode = "undirected",
                  weighted = TRUE)
})
```

(because the previous target was not interactive we need to load the graph manually once it is available)

```{r load_graph}
tar_noninteractive(tar_load(div.graph))
```

And finally extract the communities:

```{targets div.groups, tar_simple=TRUE}
cluster_louvain(div.graph)
```

## Compute projections

### *t*-SNE

Rather than creating a projection based on the features, we will create a projection based on the geodesic distance between cells:

```{targets div.graph.dist, tar_interactive=FALSE}
tar_target(div.graph.dist,
           distances(div.graph,
                     weights = NA))
```

We can then use *t*-SNE to project the graph:

```{targets div.graph.tsne, tar_interactive=FALSE}
tar_target(div.graph.tsne,
           Rtsne(as.dist(div.graph.dist), 
                 perplexity=50, 
                 check_duplicates = FALSE,
                 is_distance = TRUE))
```

### Graphviz

Use Graphviz to project the graph in the context of the features used to build it:

```{targets gv, tar_interactive=FALSE}
tar_target(gv,{
  features <- with(markers,desc[type=="functional"])
  features <- unname(features)
  .gv <- do.optimGraphviz(div.df %>% 
                            ungroup() %>% 
                            select(all_of(features)) %>% 
                            mutate(across(all_of(features),
                                          as.numeric),
                                   across(all_of(features),
                                          do.L)) %>% 
                            as.matrix(.),
                          graph = div.graph)
  rownames(.gv) <- features
  .gv
})
```

Once channel order is optimized we can create the projection:

```{targets div.graph.gv, tar_interactive=FALSE}
tar_target(div.graph.gv,{
  do.radviz(div.df %>% 
              ungroup() %>% 
              mutate(Community = factor(membership(div.groups)[id])),
            springs = gv,
            trans = do.L)
})
```

# Stage 4 - exploratory analysis

## Community size

To compute the community size per sample, we start by specifying the variables of interest that should be used for the grouping:

```{targets params_vars, tar_simple=TRUE}
c("Treatment","Individual")
```

Next we compute the community size and transform it into a *clr* value:

```{targets div.clr}
tar_target(div.clr,{
  N <- length(div.groups)
  
  div.df %>% 
    ungroup() %>% 
    mutate(Community = factor(membership(div.groups)[id])) %>% 
    count(across(all_of(c(params_vars,"Community")))) %>%
    complete(Community,
             nesting(select(.,all_of(params_vars))),
             fill=list(n=0)) %>% 
    group_by(across(all_of(params_vars))) %>%
    mutate(pc = n/sum(n),
           minim = 1/sum(n,1),
           m = N-sum(n!=0),
           ts = m*minim,
           pc = if_else(n==0,
                        minim,
                        pc - pc*ts),
           clr = log(pc/exp(mean(log(pc))))) %>% 
    select(-minim,-m,-ts)
})
```

## Biplot

To compute a biplot representation of the community contributions per sample, we first set the alpha parameters, which describes the relative contribution of features and samples to the projection; use 1 to focus on features, 0 to focus on samples:

```{targets params_alpha, tar_simple=TRUE}
0.5
```

We can then compute the `svd` representation of the *clr* matrix:

```{targets div.svd}
tar_target(div.svd,{
  .log.pc <- div.clr %>% 
    select(Community,Treatment,Individual,pc) %>% 
    spread(Community,pc) %>% 
    unite("Sample",Treatment,Individual, sep = "_")
  .rows <- .log.pc[,1,drop=TRUE]
  .log.pc <- as.matrix(.log.pc[,-1])
  
  ## double centered svd
  .z <- log(.log.pc)
  .z.avg <- mean(.z)
  .z.row <- rowMeans(.z)
  .z.col <- colMeans(.z)
  .z <- sweep(.z,1,.z.row,`-`)
  .z <- sweep(.z,2,.z.col,`-`)
  .z <- .z+.z.avg
  .svd <- svd(.z)
  
  ## biplot
  .f <- .svd$u %*% diag(.svd$d^params_alpha)
  .f <- .f[,seq(2)]
  .f <- tibble(x = .f[,1],
               y = .f[,2],
               Sample = .rows) %>% 
    separate(Sample,
             c("Treatment","Individual"),
             sep = "_",
             remove = FALSE)
  
  .g <- .svd$v %*% diag(.svd$d^(1-params_alpha))
  .g <- .g[,seq(2)]
  if(params_alpha==0) {
    .g <- .g/((length(.rows)-1)^0.5)
  }
  .g <- tibble(x = .g[,1],
               y = .g[,2],
               Community = colnames(.log.pc))
  list(bip_F = .f,
       bip_G = .g)
})
```

## Reports

Here we can specify which additional reports should be run using the output of the pipeline:

```{targets explore}
tarchetypes::tar_render(explore,"reports/_explore.Rmd")
```

